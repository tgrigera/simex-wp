<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-06-26 Sun 14:28 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Estimating experimental errors with resampling</title>
<meta name="author" content="Tomás S. Grigera" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Estimating experimental errors with resampling</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgbf3ab22">1. Synthetic data</a></li>
<li><a href="#org9e36b33">2. Estimation of the mean</a>
<ul>
<li><a href="#orgc2f3daa">2.1. Gaussian case</a></li>
<li><a href="#orgb3acad3">2.2. Student's t-distribution</a></li>
<li><a href="#org9bbdf87">2.3. Cauchy-Lorentz</a></li>
</ul>
</li>
<li><a href="#orgf6ea01c">3. Fit parameters</a>
<ul>
<li><a href="#orga7ae072">3.1. Gaussian errors</a></li>
<li><a href="#org122b961">3.2. Non-Gaussian errors</a></li>
</ul>
</li>
<li><a href="#org3643730">4. References</a></li>
</ul>
</div>
</div>
<p>
We test here a resampling method for error estimation, using synthetic data with known distributions.
We use the resampling procedure as described in  Leuzzi, Marinari, and Parisi <a href="#citeproc_bib_item_1">2020, chap. 5</a>.
</p>

<p>
Here we assume the experiment produces a set of (scalar) samples \(\{x_i\}\), \(i=1\ldots N\), distributed according to an unknown probability distribution \(D(x)\).  From these data we compute some function \(f(\{x_i\})\) which is our estimate of a quantity \(\Phi\) what we seek to determine from the experiment.  In the simplest case, the \(x_i\) are repeated measurements of some physical magnitude of interest, and \(f(\{x_i\})=\frac{1}{N}\sum_i x_i\) is the sample mean and \(\Phi=\mu\), i.e. we are estimating the mean of \(D(x)\), which we assume is the true value of the physical quantity.
</p>

<p>
To quantify the error on our estimate for \(\Phi\), we would like to know the probability distribution \(P(f |x_i)\), i.e. the distribution of our estimate, given that our experimental results are what they are.
</p>

<p>
The resampling method builds an estimate of \(P(f|x_i)\) by computing \(f\) on many artificial experimental sets.  The artificial sets are constructed with the prescription that each of the original data points are picked with probability 1/2 (or put in other words, each data point is reweighed with a weight that is 0 or 1 with equal probability).
</p>

<p>
The procedure is implemented in the following <code>Julia</code> function:
</p>
<div class="org-src-container">
<pre class="src src-jupyter-julia"><span style="color: #3a81c3; font-weight: bold;">using</span> Statistics

<span style="color: #2d9574;">"""Do `Nrs` resamplings of data vector `X` and return the set of</span>
<span style="color: #2d9574;">   values resulting from applying `f` to each resampling."""</span>
<span style="color: #3a81c3; font-weight: bold;">function</span> <span style="color: #6c3163; font-weight: bold;">resample</span>(X,Nrs,f=Statistics.mean,RNG=Random.GLOBAL_RNG)
    fs=[]
    <span style="color: #3a81c3; font-weight: bold;">for</span> _<span style="color: #3a81c3; font-weight: bold;">=</span>1:Nrs
        rsamp=eltype(X)[]
        <span style="color: #3a81c3; font-weight: bold;">for</span> x <span style="color: #3a81c3; font-weight: bold;">in</span> X <span style="color: #3a81c3; font-weight: bold;">if</span> Random.rand(RNG,Float64)&gt;0.5 push!(rsamp,x) <span style="color: #3a81c3; font-weight: bold;">end</span> <span style="color: #3a81c3; font-weight: bold;">end</span>
        push!(fs,f(rsamp))
    <span style="color: #3a81c3; font-weight: bold;">end</span>
    <span style="color: #3a81c3; font-weight: bold;">return</span> fs
<span style="color: #3a81c3; font-weight: bold;">end</span>
</pre>
</div>

<pre class="example">
resample
</pre>

<div id="outline-container-orgbf3ab22" class="outline-2">
<h2 id="orgbf3ab22"><span class="section-number-2">1.</span> Synthetic data</h2>
<div class="outline-text-2" id="text-1">
<p>
To test the above procedure we will generate pseudo-random data sets centred around zero and distributed according to the following well-known distributions:
</p>
\begin{align*}
D_G(x;\mu,\sigma) &= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[\frac{(x-\mu)^2}{2\sigma^2}\right], & \text{Gaussian}\\
D_S (x; \nu) &= \frac{\Gamma( \frac{\nu+1}{2})}{\sqrt{\nu\pi} \Gamma(\nu/2)} \left( 1 + \frac{x^2}{\nu} \right)^{-\frac{\nu + 1}{2}}, & \text{Student's t} \\
D_c (x; \mu, \sigma) &= \frac{1}{\pi \sigma} \frac{1}{ 1 + \left(\frac{x -\mu}{\sigma} \right)^2 }, & \text{Cauchy-Lorentz}
\end{align*}
<p>
We shall set \(\mu=0\) and \(\sigma=2\) for the Gaussian and Cauchy-Lorentz distributions, and use two values of \(\nu\) for the Student's t-distribution, so that the first moments are:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-right">Mean</td>
<td class="org-left">Variance</td>
</tr>

<tr>
<td class="org-left">Gaussian &sigma;=2</td>
<td class="org-right">0</td>
<td class="org-left">4</td>
</tr>

<tr>
<td class="org-left">Student &nu;=8/3</td>
<td class="org-right">0</td>
<td class="org-left">4</td>
</tr>

<tr>
<td class="org-left">Student &nu;=1.1</td>
<td class="org-right">0</td>
<td class="org-left">undef</td>
</tr>

<tr>
<td class="org-left">Cauchy-Lorentz &sigma;=2</td>
<td class="org-right">undef</td>
<td class="org-left">undef</td>
</tr>
</tbody>
</table>
<p>
Student's-t with \(\nu=1.1\) has mean but no variance, while the Cauchy-Lorentz distribution does not even have a defined mean (it's median is equal to \(\mu\), or zero in our case).  We can generate pseud-random numbers with these distributions with <code>Julia</code>'s <code>Distributions</code> package.
</p>
<div class="org-src-container">
<pre class="src src-jupyter-julia"><span style="color: #3a81c3; font-weight: bold;">using</span> Random,Distributions

&#963;=2
&#963;sq=&#963;^2
gauss=Normal(0,&#963;)
&#957;=2&#963;sq/(&#963;sq-1)
student_narrow=TDist(&#957;)
&#957;w=1.1
student_wide=TDist(&#957;w)
cauchy=Cauchy(0,&#963;)
</pre>
</div>

<pre class="example">
Cauchy{Float64}(μ=0.0, σ=2.0)
</pre>


<p>
This is how the distributions look.  Points are histograms computed from a set of 10<sup>5</sup> random numbers.  We usually will not have the luxury of such a large data set in experiments, but for now it serves as a check that our pseudo-random numbers are what we need them to be.
</p>


<div id="org840028e" class="figure">
<p><img src="./ob-jupyter/e4fbd913a706daa2d35f376221a066f5ffdcff27.svg" alt="e4fbd913a706daa2d35f376221a066f5ffdcff27.svg" class="org-svg" />
</p>
</div>
</div>
</div>


<div id="outline-container-org9e36b33" class="outline-2">
<h2 id="org9e36b33"><span class="section-number-2">2.</span> Estimation of the mean</h2>
<div class="outline-text-2" id="text-2">
<p>
Let's try the resampling method to put a confidence interval on the estimation of the mean.  Below we draw synthetic samples of \(N=10\) to 5000 data points, compute the average of those as an estimate of the mean.  We then do as many resamplings as there are data points, then compute the population standard deviation of the means of each resampling.  We give in the tables below the estimation of &mu; with a confidence interval of twice the standard deviation, the estimation of the s.d. of the mean, and the simple estimate of the s.d. using the population s.d divided by \sqrt{N},
\[ \left\{ \frac{1}{N(N-1)} \sum_i \left( x_i-\overline{x}\right) \right\}^{1/2}.\]
</p>
</div>


<div id="outline-container-orgc2f3daa" class="outline-3">
<h3 id="orgc2f3daa"><span class="section-number-3">2.1.</span> Gaussian case</h3>
<div class="outline-text-3" id="text-2-1">
<p>
The Gaussian case is the simplest, here we expect the simple estimation to work well, plus we know exactly the s.d. of the estimation of \(\mu\) (it is \(\sigma/\sqrt{n}\)).  We produce the table with the following code:
</p>

<div class="org-src-container">
<pre class="src src-jupyter-julia"><span style="color: #3a81c3; font-weight: bold;">using</span> Printf

println(<span style="color: #2d9574;">"|    N |   &#956; estimation |   &#963; est | &#963; naive est | Actual &#963; |"</span>)
<span style="color: #3a81c3; font-weight: bold;">for</span> Ns <span style="color: #3a81c3; font-weight: bold;">in</span> [10,20,50,100,200,500,1000,2000,5000]
    A=rand(gauss,Ns)
    &#956;_e=Statistics.mean(A)
    rA=resample(A,Ns)
    &#963;_e=sqrt(var(rA))
    <span style="color: #6c3163;">@printf</span> <span style="color: #2d9574;">"| %4d | %7.4f&#177;%6.4f | %7.4f |     %7.4f |  %7.4f | \n"</span> Ns &#956;_e 2&#963;_e &#963;_e sqrt(var(A)/(Ns-1)) &#963;/sqrt(Ns)
<span style="color: #3a81c3; font-weight: bold;">end</span>
</pre>
</div>

<pre class="example">
|    N |   μ estimation |   σ est | σ naive est | Actual σ |
|   10 | -0.0930±1.2391 |  0.6196 |      0.6993 |   0.6325 | 
|   20 |  0.0966±0.9641 |  0.4821 |      0.4271 |   0.4472 | 
|   50 |  0.0011±0.5113 |  0.2556 |      0.2511 |   0.2828 | 
|  100 |  0.3426±0.3665 |  0.1832 |      0.1837 |   0.2000 | 
|  200 | -0.0676±0.2800 |  0.1400 |      0.1494 |   0.1414 | 
|  500 |  0.0061±0.1737 |  0.0869 |      0.0867 |   0.0894 | 
| 1000 |  0.0160±0.1400 |  0.0700 |      0.0664 |   0.0632 | 
| 2000 | -0.0049±0.0886 |  0.0443 |      0.0443 |   0.0447 | 
| 5000 | -0.0378±0.0547 |  0.0274 |      0.0279 |   0.0283 | 
</pre>
</div>
</div>


<div id="outline-container-orgb3acad3" class="outline-3">
<h3 id="orgb3acad3"><span class="section-number-3">2.2.</span> Student's t-distribution</h3>
<div class="outline-text-3" id="text-2-2">
<p>
First we do, with similar code as above, the "narrow" t-distribution i.e. with finite variance equal to 4, as in the Gaussian case.
</p>

<pre class="example">
|    N |   μ estimation |   σ est | σ naive est |
|   10 |  1.1900±1.6914 |  0.8457 |      0.7954 |
|   20 | -0.1149±0.7712 |  0.3856 |      0.2818 |
|   50 | -0.5587±0.7726 |  0.3863 |      0.3429 |
|  100 |  0.0011±0.3239 |  0.1619 |      0.1723 |
|  200 |  0.0948±0.4260 |  0.2130 |      0.2016 |
|  500 |  0.0535±0.3885 |  0.1942 |      0.1984 |
| 1000 |  0.0365±0.1171 |  0.0586 |      0.0581 |
| 2000 | -0.0260±0.0859 |  0.0429 |      0.0427 |
| 5000 | -0.0295±0.0555 |  0.0278 |      0.0280 |
</pre>


<p>
Now the t-distribution with undefined variance.  Things are not good here, in the sense that the error is not decreasing systematically with more samples.  However, both the resampling and naive estimations of standard deviation warn us about that.
</p>

<pre class="example">
|    N |    μ estimation |   σ est | σ naive est |
|   10 |  2.5102± 6.4034 |  3.2017 |      3.4710 |
|   20 |  1.1654± 4.8692 |  2.4346 |      2.7253 |
|   50 | -2.3050± 3.0301 |  1.5150 |      1.5365 |
|  100 |  2.2331± 4.3243 |  2.1621 |      2.1756 |
|  200 |  0.3414± 0.7339 |  0.3670 |      0.4036 |
|  500 | -1.0585± 3.6381 |  1.8191 |      1.8011 |
| 1000 | -0.0966± 2.2454 |  1.1227 |      1.1519 |
| 2000 |  1.5855± 4.0281 |  2.0141 |      2.0182 |
| 5000 | 32.6215±68.3293 | 34.1647 |     34.1605 |
</pre>
</div>
</div>


<div id="outline-container-org9bbdf87" class="outline-3">
<h3 id="org9bbdf87"><span class="section-number-3">2.3.</span> Cauchy-Lorentz</h3>
<div class="outline-text-3" id="text-2-3">
<p>
This distribution doesn't have a mean, so it's not surprising that errors are not decreasing with \(N\) here.
</p>

<pre class="example">
|    N |   μ estimation |    σ est | σ naive est |
|   10 | -0.5017± 4.9469 |  2.4735 |      1.7552 |
|   20 | -1.2743± 7.6128 |  3.8064 |      3.7377 |
|   50 |  5.5722±10.7263 |  5.3631 |      5.9061 |
|  100 | -2.3583± 4.0697 |  2.0349 |      2.0916 |
|  200 |  0.3391± 3.1496 |  1.5748 |      1.5831 |
|  500 | -0.2632± 7.1307 |  3.5653 |      3.7054 |
| 1000 |  0.2794± 2.5352 |  1.2676 |      1.2566 |
| 2000 | 12.3297±20.5714 | 10.2857 |     10.2826 |
| 5000 |  0.1749± 2.4781 |  1.2390 |      1.2605 |
</pre>


<p>
Things work much better with the <i>median</i>.  The naive error estimate has no meaning here, but the resampling estimate is satisfactory.
</p>

<pre class="example">
|    N |   μ estimation |   σ est | σ naive est |
|   10 | -1.2699± 2.7900 |  1.3950 |      0.8036 | 
|   20 | -0.6718± 0.7847 |  0.3924 |      4.9249 | 
|   50 | -0.6651± 1.1596 |  0.5798 |      3.7524 | 
|  100 |  0.3848± 0.7916 |  0.3958 |      1.1779 | 
|  200 |  0.0793± 0.3939 |  0.1969 |      2.3231 | 
|  500 | -0.0357± 0.3547 |  0.1773 |     87.3314 | 
| 1000 |  0.0540± 0.2017 |  0.1009 |      4.1377 | 
| 2000 |  0.1120± 0.1353 |  0.0676 |      8.3017 | 
| 5000 | -0.0269± 0.0814 |  0.0407 |      0.6318 | 
</pre>


<p>
The plots of the distributions below illustrate a bit what is going on.
</p>


<div id="orgd8fb15d" class="figure">
<p><img src="./ob-jupyter/f83dfdb1ce77e6adb009d358fd57360d2b9dc6a3.svg" alt="f83dfdb1ce77e6adb009d358fd57360d2b9dc6a3.svg" class="org-svg" />
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-orgf6ea01c" class="outline-2">
<h2 id="orgf6ea01c"><span class="section-number-2">3.</span> Fit parameters</h2>
<div class="outline-text-2" id="text-3">
<p>
How does resampling work when trying to put an error to parameters obtained from curve-fitting?  We try resampling on fake experimental data created by adding noise to a power law \(y=ax^z\).  We'll use the next function to draw a simulated dataset with errors around the power law, leaving the error distribution to be decided upon later.  We add noise both on \(x\) and \(y\) and discard errors that would make values negative (this we want from a power law with real exponent, plus, we'll use the logarithms of the data points to do the fitting).
</p>
<div class="org-src-container">
<pre class="src src-jupyter-julia" id="org0888710"><span style="color: #3a81c3; font-weight: bold;">function</span> <span style="color: #6c3163; font-weight: bold;">draw_dataset</span>(a,z,N,xerror,yerror;logxmin=-2,logxmax=-1)
    x=10 .^range(logxmin,logxmax,length=N)
    y=a*x.^z
    xe=-ones(size(x))
    ye=-ones(size(y))
    <span style="color: #3a81c3; font-weight: bold;">for</span> i<span style="color: #3a81c3; font-weight: bold;">=</span>1:length(x)
        <span style="color: #3a81c3; font-weight: bold;">while</span> xe[i]&lt;0 xe[i]=x[i]+xerror(x[i]) <span style="color: #3a81c3; font-weight: bold;">end</span>
        <span style="color: #3a81c3; font-weight: bold;">while</span> ye[i]&lt;0 ye[i]=y[i]+yerror(y[i]) <span style="color: #3a81c3; font-weight: bold;">end</span>
    <span style="color: #3a81c3; font-weight: bold;">end</span>
    <span style="color: #3a81c3; font-weight: bold;">return</span> x,y,xe,ye
<span style="color: #3a81c3; font-weight: bold;">end</span>
</pre>
</div>

<pre class="example">
draw_dataset (generic function with 1 method)
</pre>
</div>


<div id="outline-container-orga7ae072" class="outline-3">
<h3 id="orga7ae072"><span class="section-number-3">3.1.</span> Gaussian errors</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Let's start with a 40-point dataset obtained using \(a=10\), \(z=1.2\), and <i>relative</i> errors distributed with a Gaussian with \(\sigma=0.2\) in both coordinates.
</p>
<div class="org-src-container">
<pre class="src src-jupyter-julia">Random.seed!(1125)
actual_A=10; actual_z=1.2
Npoints=40
error_xdist=Normal(0,0.2)
error_ydist=Normal(0,0.2)
<span style="color: #6c3163; font-weight: bold;">errx</span>(x) = x*rand(error_xdist)
<span style="color: #6c3163; font-weight: bold;">erry</span>(y) = y*rand(error_ydist)
x,y,xe,ye = draw_dataset(actual_A,actual_z,Npoints,errx,erry);
</pre>
</div>

<p>
We'll try  either linear fit of the logarithms or a nonlinear fit.  Since we are assuming uniform relative errors, for an unweighted fit it makes more sense to use the logarithms.
</p>
<div class="org-src-container">
<pre class="src src-jupyter-julia" id="org65e8274"><span style="color: #3a81c3; font-weight: bold;">function</span> <span style="color: #6c3163; font-weight: bold;">fitgp_log</span>(x,y)
  <span style="color: #6c3163;">@gp</span> <span style="color: #2d9574;">"\$fitdata"</span>=&gt;(log10.(x),log10.(y)) :-
  <span style="color: #6c3163;">@gp</span> :- <span style="color: #2d9574;">"f(x) = z*x+logA"</span> :-
  <span style="color: #6c3163;">@gp</span> :- <span style="color: #2d9574;">"fit f(x) \$fitdata via z,logA"</span> :-
  vars=gpvars()
  <span style="color: #3a81c3; font-weight: bold;">return</span> vars.z,10^vars.logA,vars.z_err
<span style="color: #3a81c3; font-weight: bold;">end</span>

<span style="color: #3a81c3; font-weight: bold;">function</span> <span style="color: #6c3163; font-weight: bold;">fitgp_nl</span>(x,y)
  <span style="color: #6c3163;">@gp</span> <span style="color: #2d9574;">"\$fitdata"</span>=&gt;(x,y) :-
  <span style="color: #6c3163;">@gp</span> :- <span style="color: #2d9574;">"f(x) = A*x**z"</span> :-
  <span style="color: #6c3163;">@gp</span> :- <span style="color: #2d9574;">"fit f(x) \$fitdata via z,A"</span> :-
  vars=gpvars()
  <span style="color: #3a81c3; font-weight: bold;">return</span> vars.z,vars.A,vars.z_err
<span style="color: #3a81c3; font-weight: bold;">end</span>

<span style="color: #3a81c3; font-weight: bold;">using</span> Polynomials
<span style="color: #3a81c3; font-weight: bold;">function</span> <span style="color: #6c3163; font-weight: bold;">fitjl_log</span>(x,y)
    f=Polynomials.fit(log10.(x),log10.(y),1)
    <span style="color: #3a81c3; font-weight: bold;">return</span> coeffs(f)[2],10^coeffs(f)[1],0
<span style="color: #3a81c3; font-weight: bold;">end</span>
</pre>
</div>

<pre class="example">
fitjl_log (generic function with 1 method)
</pre>


<p>
This is how our noisy dataset looks like together with the fits.  From now on we shall only use the log-linear fit with the Julia code, which does not give errors but is faster.
</p>
<pre class="example">
From linear fit of the logs (gnuplot): z = 1.13868780170677, s.d=0.0638290280504025
From linear fit of the logs (Julia): z = 1.138687795950956
From nonlinear fit: z = 0.945143141074797, s.d=0.0847012187377099
</pre>


<div id="org6f9ab22" class="figure">
<p><img src="./ob-jupyter/9cebb94ac5e4c4097d1021a1e3803e285c0b00a2.svg" alt="9cebb94ac5e4c4097d1021a1e3803e285c0b00a2.svg" class="org-svg" />
</p>
</div>

<p>
Now we shall compute the distribution of \(z\) using the resampling method with the dataset shown above, and compare it with the distribution obtained from drawing many datasets with the same noise distribution.
</p>
<div class="org-src-container">
<pre class="src src-jupyter-julia" id="org14ed686"><span style="color: #3a81c3; font-weight: bold;">function</span> <span style="color: #6c3163; font-weight: bold;">redraw_fit</span>(Nsamp,drawfunction,fitfunction)
    zs=Float64[]
    <span style="color: #3a81c3; font-weight: bold;">for</span> _<span style="color: #3a81c3; font-weight: bold;">=</span>1:Nsamp
        x,y,xe,ye = drawfunction()
        z,A,z_err=fitfunction(xe,ye)
        push!(zs,z)
    <span style="color: #3a81c3; font-weight: bold;">end</span>
    <span style="color: #3a81c3; font-weight: bold;">return</span> zs
<span style="color: #3a81c3; font-weight: bold;">end</span>

<span style="color: #3a81c3; font-weight: bold;">function</span> <span style="color: #6c3163; font-weight: bold;">resample_fit</span>(xe,ye,Nrs,fitfunction)
    zs=Float64[]
    <span style="color: #3a81c3; font-weight: bold;">for</span> _<span style="color: #3a81c3; font-weight: bold;">=</span>1:Nrs
        xrs=Float64[]
        yrs=Float64[]
        <span style="color: #3a81c3; font-weight: bold;">for</span> i <span style="color: #3a81c3; font-weight: bold;">in</span> eachindex(xe) <span style="color: #3a81c3; font-weight: bold;">if</span> Random.rand(Float64)&gt;0.5 push!(xrs,xe[i]); push!(yrs,ye[i]) <span style="color: #3a81c3; font-weight: bold;">end</span> <span style="color: #3a81c3; font-weight: bold;">end</span>
        z,A,z_err=fitfunction(xrs,yrs)
        push!(zs,z)
    <span style="color: #3a81c3; font-weight: bold;">end</span>
    <span style="color: #3a81c3; font-weight: bold;">return</span> zs
<span style="color: #3a81c3; font-weight: bold;">end</span>
</pre>
</div>

<pre class="example">
resample_fit (generic function with 1 method)
</pre>


<pre class="example">
sd from resampling = 0.07696983166805006
sd from redrawing  = 0.07058433611134453
</pre>


<div id="org466f737" class="figure">
<p><img src="./ob-jupyter/f2ddc6ec2be518b29486587bc27e98e9aee5f442.svg" alt="f2ddc6ec2be518b29486587bc27e98e9aee5f442.svg" class="org-svg" />
</p>
</div>

<p>
Now we repeat the procedure but without errors in x
</p>
<pre class="example">
Fit of one sample:  z=1.23549774078608, s.d = 0.0392945637464569
sd from resampling = 0.04663230336560641
sd from redrawing  = 0.049139038124389696
</pre>


<div id="org44f4d61" class="figure">
<p><img src="./ob-jupyter/92e4ca04eea6112bbb71dbc2d81955710b134681.svg" alt="92e4ca04eea6112bbb71dbc2d81955710b134681.svg" class="org-svg" />
</p>
</div>

<p>
<b>In summary:</b> for Gaussian errors, the resampling and Gnuplot s.d. estimates are quite good.  The unweighted fit seems to introduce a <i>bias</i> towards smaller exponents in the case of uniform relative errors in x.
</p>
</div>
</div>


<div id="outline-container-org122b961" class="outline-3">
<h3 id="org122b961"><span class="section-number-3">3.2.</span> Non-Gaussian errors</h3>
<div class="outline-text-3" id="text-3-2">
<p>
We now try to use resampling using a broader distribution for the errors.   We'll use again  40 points, \(y=a x^z\) with \(z=1.2\), then add noise to them using a rescaled Student's distribution with undefined variance:
</p>
<div class="org-src-container">
<pre class="src src-jupyter-julia">Random.seed!(1125)
actual_A=10; actual_z=1.2
Npoints=40

&#957;x=1.1
xerror_dist=TDist(&#957;x)
&#957;y=1.1
yerror_dist=TDist(&#957;y)
<span style="color: #6c3163; font-weight: bold;">xerror</span>(x) = 0.1*x*rand(xerror_dist)
<span style="color: #6c3163; font-weight: bold;">yerror</span>(y) = 0.2*y*rand(yerror_dist)
x,y,xe,ye = draw_dataset(actual_A,actual_z,Npoints,xerror,yerror);
</pre>
</div>

<pre class="example">
From linear fit of the logs (gnuplot): z = 1.02884962690192, s.d=0.123669598850099
From linear fit of the logs (Julia): z = 1.028849480357536
From nonlinear fit: z = 1.15350070116037, s.d=0.141987553786144
</pre>


<div id="org6bf2dd8" class="figure">
<p><img src="./ob-jupyter/db269b215033b5b024c3f2f760c35cfb1de92940.svg" alt="db269b215033b5b024c3f2f760c35cfb1de92940.svg" class="org-svg" />
</p>
</div>

<p>
The distribution of exponents with resampling and with redrawing.  The bias towards smaller exponents is also seen.  s.d. from resampling is smaller that actual s.d.
</p>
<pre class="example">
sd from resampling = 0.1149518870663573
sd from redrawing  = 0.20946558038211066
</pre>


<div id="org8f9972f" class="figure">
<p><img src="./ob-jupyter/3e7628d1e0bd9840071b2486efceabbf770d74d3.svg" alt="3e7628d1e0bd9840071b2486efceabbf770d74d3.svg" class="org-svg" />
</p>
</div>

<p>
Now without errors in the abscissas.  Here there is no bias, and the resampling sd estimate is quite reasonable. 
</p>
<pre class="example">
Fit of one sample:  z=1.25460546653145, s.d = 0.0935657648424533
sd from resampling = 0.11234109127420418
sd from redrawing  = 0.13691383411049807
</pre>


<div id="orge76b7ff" class="figure">
<p><img src="./ob-jupyter/81f804f73a4d1458ea5828d613463cd501333ebe.svg" alt="81f804f73a4d1458ea5828d613463cd501333ebe.svg" class="org-svg" />
</p>
</div>
</div>
</div>
</div>



<div id="outline-container-org3643730" class="outline-2">
<h2 id="org3643730"><span class="section-number-2">4.</span> References</h2>
<div class="outline-text-2" id="text-4">
<style>.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>Leuzzi, Luca, Enzo Marinari, and Giorgio Parisi. 2020. <i>Trattatello Di Probabilità</i>. Roma: web only.</div>
</div>


<p>

</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: Jun 19, 2022</p>
<p class="author">Author: Tomás S. Grigera</p>
<p class="date">Created: 2022-06-26 Sun 14:28</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
